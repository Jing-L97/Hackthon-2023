{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp467o9RLXZh"
   },
   "source": [
    "Step 1: load dataset and extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.5.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.10.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (67.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-md==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (67.5.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing: keywords extraction\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "6kA3y-1lLCCi"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "hlbItqAp4e0Z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296,
     "referenced_widgets": [
      "b1b2b968a8404b5bae6852daaa7e1842",
      "7116877e2a65485eb6ba623145c3a60c",
      "01ac11c3a5ef480a86a52447805e246e",
      "02f7c679d2254de7b14a2c2f2ff7730e",
      "3c52745217a549259f2e541b979ded0a",
      "0aedf9efff6945a786a0f9997bb8d303",
      "cd7d84505227429ca1f95979fc314220",
      "2718a4c8035344808c6a4306a46bbaf0",
      "6bcdbc3837f043bba6fabd040649b3a9",
      "1738e50f32af42659c5f30095e0a9a9f",
      "eeac502e498d4fc0af425a3167e1adec",
      "78499f2bf4fa4e2c9bc1f46e26c3b0d3",
      "f9f78424003148259f174e87854db31b",
      "e026bd7a6cf64ee28f14f41ef32ada39",
      "1c40e33aa2964282844255726e1f32f0",
      "e936e99448944773bed2e5df3585f6a8",
      "1674de6009ee48fdb2dcb3e2bdc5de25",
      "3baec797f750480bb2860082e224292d",
      "8f59d873ba744361a369c40bfa2118b0",
      "1898feb24c944a6bb23d9e35a4b3cce3",
      "404d957f52f74628b55b2d335a295dc3",
      "1c086f2b96204a85b384964abdcd1966",
      "3373ad07f1a0452c883f5c08f732b435",
      "adf4c527b4714a3f9fec38ed0cca2712",
      "4955d45ee0af4eedb0e822816eedb996",
      "c7164c1aa095486f875d624805272853",
      "35d5a7eb6c4443719fbfbfc582012f1c",
      "7fda9e42c12b4f71b4408fa598a0ecdc",
      "2f0af798f7964180b47dae901f6f3883",
      "7db1afd395814018989bbaac88a6d5e0",
      "2bdd6896bd434153bfa72dc9c76b0f8e",
      "c05e98aef5144035b7f0aec0b4c8a90e",
      "949f692d5ecb4cf7acf61f7ab76695ed",
      "d8abed034b1644cfa043757ccc545294",
      "f0c51421e7af43e299cdf45c249ebc82",
      "0c2fbe5caa7248b3a6e78a071efffcde",
      "5d5d8533b9634e74b7032707f5b6830e",
      "e008eb9fda934ae2a8f3a48028b13b21",
      "9b32f88f5cbe43f3a77b5a9b0284cf72",
      "613f7a20b5cf4164b6f396b2072a6b09",
      "14d0e6da556b45a38da93d7e2414266c",
      "76999930692a466b8fc0762395736d95",
      "e60f007d6b6e4b6bac6d5e71e120227e",
      "fa093ba8146b4f8f881480b55a70b557",
      "5c04213aafd04d189a3b86a89f99e609",
      "5891840a488e4ce8bd9f3de60abac1a9",
      "2191cf040f1e44209d7591a63468a713",
      "212ecc984a194f46825956a1b0e77c86",
      "9b81c93801174f15b5d79119c318acb0",
      "ebeb8c21d02b423fb1ede920b212839a",
      "78b64135675d4607ad636764b1e148ce",
      "65065ffbfcf74b6593dba16810f2c967",
      "d3cbaa0a7a5d437883fb778f30f0d3d5",
      "32861319e8214cbfad2068fda77c50a9",
      "7103ddc064294982a3d31af851286bed",
      "c7e408b64fe840e981335abf82f9ed7d",
      "854d3efe07c949bca74f39be92dfd5b3",
      "f4df1cea35a94a269e32e79b8d955467",
      "ffd923ac5f1c4b8a977afddc36e14176",
      "68d417eef3d6497a9ce3ba046ae26ff6",
      "49491706e1784c20adb5c0ad084464b0",
      "461a4d79c9ba4dcaa95f7eb8da32e497",
      "a6ef6ccdc4be4e11a0aff5f6e6d1a337",
      "df6c2cc6ef9e408692f5e9016282533e",
      "8eada4718cc045bf91b2b4cb554716b1",
      "c8f73487e3e14d2aad3ca8b89fd00e91",
      "fee85683fb28489caf73255208329b06",
      "7b9511e206e642ee86ebb65ac1d4f073",
      "b83160c71bab4d30a7b08b42f0d37c2e",
      "7e59d76f67834323982fdf980dd8366d",
      "f52ad17356e7448cac136a4097e58b4a",
      "a876f5453a18415587ecbe3d52478e5d",
      "47a4879b03374a2dbf90d780a687a79d",
      "5149a9b5971f468b910a7910c69c9b7e",
      "9b8635bff5f342e88d12c3cdc92898f9",
      "12dce0a5df5d4e27abdfbeacd4da1b0a",
      "2b2881f0d3ed415b82b9e870b665fed8",
      "dacaf8247eb74fa0a4dafa1d6a8d742a",
      "d061bcc0c20543a0beb3da42efddbcfe",
      "004306a940d34fc7979b6b758ee603b8",
      "5bf79297722b49a3ad22437b5fa570a3",
      "e0a3dbc0b71a4802b59d75f8700f2e09",
      "13d545b4a94942f08da04267c405f6e2",
      "685893d229ec4df8ad038880db9c665b",
      "acbb723270334977ac9f9504024b6034",
      "6c0068b12a3548fb979aee34a10dddb5",
      "c5f1295622064b5e8b780a4976130786",
      "c8ff5523a9de45b3b108e246e1c2160f"
     ]
    },
    "id": "Y3_viIdr4e7W",
    "outputId": "94ac9f7d-fb27-4b07-aaac-c309a073d784"
   },
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv(\"gs://novhack_2023_training_data/X_train.csv\", index_col=0)\n",
    "\n",
    "# remove nan rows\n",
    "raw_dataset.dropna(subset=['samples'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword(text, condi, number):\n",
    "    # Tokenize the text\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  # Convert to lowercase\n",
    "  tokens = [token.lower() for token in tokens]\n",
    "\n",
    "  # Remove stopwords and punctuations\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "\n",
    "  # Lemmatize tokens\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Create a frequency distribution of the tokens\n",
    "  freq_dist = FreqDist(tokens)\n",
    "\n",
    "  # Calculate the total number of documents and the document frequency of each token\n",
    "  num_docs = 1\n",
    "  doc_freq = {}\n",
    "  for token in set(tokens):\n",
    "      doc_freq[token] = 1\n",
    "\n",
    "  # Calculate the TF-IDF scores\n",
    "  tfidf_scores = {}\n",
    "  for token in set(tokens):\n",
    "      tf = freq_dist[token] / len(tokens)\n",
    "      idf = math.log(num_docs / doc_freq[token])\n",
    "      tfidf_scores[token] = tf * idf\n",
    "  \n",
    "  # Select the top n keywords in proportion with the text length\n",
    "  # for different conditions\n",
    "  if condi == 'fixed':\n",
    "    # if there are the text length is lower than the set words, just take the whole text\n",
    "    n = min([number,len(text)])\n",
    "\n",
    "  elif condi == 'prop':\n",
    "    # use logarithm transformation to reduce # keywords differences\n",
    "    n = round(len(math.log(text, 2)) * number)\n",
    "    \n",
    "  top_n = sorted(tfidf_scores, key=tfidf_scores.get, reverse=True)[:n]\n",
    "  # convert the result into a string   \n",
    "  # possible problem: discrete \n",
    "  string = ' '.join(top_n)\n",
    "  return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the extracted keywords as an additional column\n",
    "keyword_lst = []\n",
    "log_lst= []\n",
    "for text in raw_dataset['samples'].tolist():\n",
    "  try:\n",
    "    keywords = get_keyword(text, 'fixed', 10)\n",
    "    keyword_lst.append(keywords)\n",
    "  except:\n",
    "    log_lst.append(text)\n",
    "raw_dataset['keywords'] = keyword_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After turning the ignition switch off, waiting...</td>\n",
       "      <td>proceeding see read rear airbag battery split ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the vehicle stationary, the yaw rate sens...</td>\n",
       "      <td>previously value airbag read improperly proper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rack and pinion power steering gear assembly m...</td>\n",
       "      <td>value proceed ground power wait disconnect rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clear the DTCs (even if no DTCs are stored, pe...</td>\n",
       "      <td>value read trouble le power wait amount discon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After turning the ignition switch off, waiting...</td>\n",
       "      <td>value ready read operated proceed dc power act...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             samples  \\\n",
       "0  After turning the ignition switch off, waiting...   \n",
       "1  With the vehicle stationary, the yaw rate sens...   \n",
       "2  Rack and pinion power steering gear assembly m...   \n",
       "3  Clear the DTCs (even if no DTCs are stored, pe...   \n",
       "4  After turning the ignition switch off, waiting...   \n",
       "\n",
       "                                            keywords  \n",
       "0  proceeding see read rear airbag battery split ...  \n",
       "1  previously value airbag read improperly proper...  \n",
       "2  value proceed ground power wait disconnect rec...  \n",
       "3  value read trouble le power wait amount discon...  \n",
       "4  value ready read operated proceed dc power act...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first few lines\n",
    "raw_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX-bziYmLir9"
   },
   "source": [
    "Step2: convert raw texts into word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9JfBr4dFRX7",
    "outputId": "55bbbb1e-bfa4-4cae-8827-a5583f268d57"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the GloVe model\n",
    "nlp = spacy.load('en_core_web_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "cy2yBvIWGbGu"
   },
   "outputs": [],
   "source": [
    "# get sent embeddings\n",
    "def glove_embedding(X_lst):\n",
    "  vec_lst = []\n",
    "  for sent in X_lst:\n",
    "\n",
    "    # Get the sentence embedding by averaging the word embeddings\n",
    "    doc = nlp(sent)\n",
    "    embedding = doc.vector\n",
    "    vec_lst.append(embedding)\n",
    "  return np.array(vec_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert keywords into glove embeddings\n",
    "X_train = glove_embedding(keyword_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11321, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdAwnbFLLtB5"
   },
   "source": [
    "Step3: Employ different search strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "K7fDYI0wO2P1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "eJZq3zGRPOw-"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "YnROJ8lDy9Zo"
   },
   "outputs": [],
   "source": [
    "def uncertainty_search(X_unlabeled,k,clf):\n",
    "  probas = clf.predict_proba(X_unlabeled)\n",
    "\n",
    "  # Calculate the uncertainty score for each sample\n",
    "  uncertainty_scores = np.max(probas, axis=1)\n",
    "\n",
    "  # Sort the samples by their uncertainty scores in descending order\n",
    "  sorted_indices = np.argsort(uncertainty_scores)[::-1]\n",
    "\n",
    "  # remove the previous labeled ones\n",
    "  \n",
    "  # Select the top k most uncertain samples for labeling\n",
    "  selected_indices = sorted_indices[:k]\n",
    "\n",
    "  return sorted_indices, selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "TywOG0-LzdY5"
   },
   "outputs": [],
   "source": [
    "def diversity_search(X_unlabeled,k,clf): \n",
    "  \n",
    "  # Calculate the pairwise distances between each pair of samples\n",
    "  distances = cosine_distances(X_unlabeled)\n",
    "\n",
    "  # Calculate the diversity score for each sample\n",
    "  diversity_scores = np.min(distances, axis=1)\n",
    "\n",
    "  # Sort the samples by their diversity scores in descending order\n",
    "  sorted_indices = np.argsort(diversity_scores)[::-1]\n",
    "\n",
    "  # Select the top k most diverse samples for labeling\n",
    "  selected_indices = sorted_indices[:k]\n",
    "\n",
    "  return sorted_indices, selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "0Y-4j898z2PF"
   },
   "outputs": [],
   "source": [
    "# not use this for the time being\n",
    "def committe_search(X_unlabeled,k,models): \n",
    "  probas_list = [model.predict_proba(X_unlabeled) for model in models]\n",
    "\n",
    "  # Calculate the disagreement score for each sample\n",
    "  disagreement_scores = np.var(probas_list, axis=0)\n",
    "\n",
    "  # Calculate the mean disagreement score for each sample\n",
    "  mean_disagreement_scores = np.mean(disagreement_scores, axis=1)\n",
    "\n",
    "  # Sort the samples by their mean disagreement scores in descending order\n",
    "  sorted_indices = np.argsort(mean_disagreement_scores)[::-1]\n",
    "\n",
    "  # Select the top k most disagreed samples for labeling\n",
    "  selected_indices = sorted_indices[:k]\n",
    "  return sorted_indices, selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "YZbFfADD0XYz"
   },
   "outputs": [],
   "source": [
    "# Assuming that you have a pre-trained text classification model called `clf`\n",
    "def entropy_search(X_unlabeled,k,clf): \n",
    "  # Calculate the predicted probabilities for each unlabeled sample in the dataset\n",
    "  probas = clf.predict_proba(X_unlabeled)\n",
    "\n",
    "  # Calculate the entropy of the predicted probabilities for each sample\n",
    "  entropy_scores = entropy(probas.T)\n",
    "\n",
    "  # Calculate the expected information gain for each sample\n",
    "  expected_information_gain = entropy_scores - np.sum(probas * np.log2(probas), axis=1)\n",
    "\n",
    "  # Sort the samples by their expected information gain in descending order\n",
    "  sorted_indices = np.argsort(expected_information_gain)[::-1]\n",
    "\n",
    "  # Select the top k samples with the highest expected information gain for labeling\n",
    "  selected_indices = sorted_indices[:k]\n",
    "  return sorted_indices, selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority voting of the selected indices\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_number(my_dict):\n",
    "  # Create a list of tuples containing the key-value pairs of the dictionary\n",
    "  my_list = list(my_dict.items())\n",
    "\n",
    "  # Define a function to extract the value of the second item in a tuple\n",
    "  def get_value(item):\n",
    "      return item[1]\n",
    "\n",
    "  # Sort the list of tuples based on the values\n",
    "  my_list.sort(key=get_value)\n",
    "\n",
    "  # Extract the sorted keys from the sorted list of tuples\n",
    "  sorted_keys = [item[0] for item in my_list]\n",
    "\n",
    "  return sorted_keys\n",
    "\n",
    "def rank_nested_lists(nested_lists):\n",
    "    \"\"\"\n",
    "    Given a list of nested lists, output a ranked list of elements based on the number\n",
    "    of overlapping sublists.\n",
    "\n",
    "    Args:\n",
    "    nested_lists (list): a list of nested lists\n",
    "\n",
    "    Returns:\n",
    "    list: a ranked list of elements based on the number of overlapping sublists\n",
    "    \"\"\"\n",
    "    flattened = [item for sublist in nested_lists for item in sublist]  # flatten the nested lists\n",
    "    element_count = {}  # dictionary to store the count of each element\n",
    "    for element in flattened:\n",
    "        count = 0\n",
    "        for sublist in nested_lists:\n",
    "            if element in sublist:\n",
    "                count += 1\n",
    "        element_count[element] = count\n",
    "    ranked_lst = get_number(element_count)\n",
    "    ranked_lst.reverse()\n",
    "    return ranked_lst\n",
    "\n",
    "def vote_index(X_train,n,clf_rf):\n",
    "  uncer_sorted, uncer_instance = uncertainty_search(X_train, n ,clf_rf)\n",
    "  entro_sorted, entro_instance = entropy_search(X_train,n,clf_rf)\n",
    "  div_sorted, div_instance = diversity_search(X_train,n,clf_rf)\n",
    "  # get the intersection of the highest indices\n",
    "  ranked_lst = rank_nested_lists([uncer_sorted,entro_sorted,div_sorted])\n",
    "  indice_lst = ranked_lst[:n]\n",
    "  return indice_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request labels from the platform\n",
    "\n",
    "def get_label(index):\n",
    "    index_str = str(index)\n",
    "    parameters = {\n",
    "    \"team_id\": \"team-11\", # example: team-1\n",
    "    \"context\": \"prod\",  # 'dev' or 'prod' context for which you are requesting labels, only prod context gives you data for the challenge, dev is to test the labelling platform\n",
    "    \"data_id\": index_str  # id of the datapoint for which you are requesting the label\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "    \"authorization\": os.environ[\"JUPYTER_TOKEN\"]\n",
    "    }\n",
    "    # Do the request\n",
    "    resp = requests.get(\"http://labelling.novhack.euranova.eu/label\", params=parameters, headers=headers)\n",
    "    resp.status_code\n",
    "    label = resp.text\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_label():\n",
    "    parameters = {\n",
    "        \"team_id\": \"team-11\",\n",
    "    }\n",
    "    headers = {\n",
    "        \"authorization\": os.environ[\"JUPYTER_TOKEN\"]\n",
    "    }\n",
    "\n",
    "    resp = requests.get(\"http://labelling.novhack.euranova.eu/previous\", params=parameters, headers=headers)\n",
    "    resp.status_code\n",
    "    # Get the count\n",
    "    labels = resp.text\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"5\":\"Power Source / Network\"}\n"
     ]
    }
   ],
   "source": [
    "label = previous_label()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "k8FG-Gtn0ycL"
   },
   "outputs": [],
   "source": [
    "# k-means\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "iFmu6i2I1Scj"
   },
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier with 100 trees\n",
    "clf_rf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "JbdCyQCcvbh5",
    "outputId": "bb97eb56-adf9-4cb1-b074-e8cab16b3eb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "my_array = np.array(X_train)\n",
    "kmeans = KMeans(n_clusters=11)\n",
    "\n",
    "# fit the k-means object to the data\n",
    "kmeans.fit(my_array)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "closest_points = [np.argpartition(np.linalg.norm(my_array - center, axis=1), 1)[:1] for center in centers]\n",
    "\n",
    "indice_lst = []\n",
    "\n",
    "for i, indices in enumerate(closest_points):\n",
    "    indice_lst.append(indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_train = X_train[indice_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3243, 7761, 2178, 6975, 8794, 8218, 10519, 3126, 5624, 5445, 1843]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indice_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 300)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request label\n",
    "label_lst = []\n",
    "for index in indice_lst:\n",
    "    label = get_label(index)\n",
    "    # update the dataset\n",
    "    label_lst.append(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Vehicle Interior\"',\n",
       " '\"Engine / Hybrid System\"',\n",
       " '\"Drivetrain\"',\n",
       " '\"Vehicle Exterior\"',\n",
       " '\"Engine / Hybrid System\"',\n",
       " '\"Vehicle Interior\"',\n",
       " '\"Vehicle Interior\"',\n",
       " '\"Vehicle Interior\"',\n",
       " '\"Steering\"',\n",
       " '\"Engine / Hybrid System\"',\n",
       " '\"Power Source / Network\"']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kmeans not successful! \n",
    "label_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the known labels of the dataset\n",
    "result = previous_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"5\":\"Power Source / Network\",\"1843\":\"Power Source / Network\",\"2178\":\"Drivetrain\",\"3126\":\"Vehicle Interior\",\"3243\":\"Vehicle Interior\",\"5445\":\"Engine / Hybrid System\",\"5624\":\"Steering\",\"6975\":\"Vehicle Exterior\",\"7761\":\"Engine / Hybrid System\",\"8218\":\"Vehicle Interior\",\"8794\":\"Engine / Hybrid System\",\"10519\":\"Vehicle Interior\"}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the labels\n",
    "requested_index_lst = []\n",
    "requested_label_lst = []\n",
    "pair_lst = result.split(',')\n",
    "for i in pair_lst:\n",
    "    index = int(i.split(':')[0].split('\"')[1])\n",
    "    label = i.split(':')[1].split('\"')[1]\n",
    "    requested_index_lst.append(index)\n",
    "    requested_label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1843, 2178, 3126, 3243, 5445, 5624, 6975, 7761, 8218, 8794, 10519]\n",
      "['Power Source / Network', 'Power Source / Network', 'Drivetrain', 'Vehicle Interior', 'Vehicle Interior', 'Engine / Hybrid System', 'Steering', 'Vehicle Exterior', 'Engine / Hybrid System', 'Vehicle Interior', 'Engine / Hybrid System', 'Vehicle Interior']\n"
     ]
    }
   ],
   "source": [
    "print(requested_index_lst)\n",
    "print(requested_label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_updated = requested_label_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# map string label list with numbers\n",
    "num_label_dict = {'Engine / Hybrid System': 0, 'Audio / Visual / Telematics': 1, 'Brake':2,\n",
    "                       'Vehicle Interior': 3, 'Suspension': 4, 'Vehicle Exterior': 5, 'Drivetrain':6,\n",
    "                       'Power Source / Network':7, 'Steering':8, 'General':9, 'ADAS / AD':10}\n",
    "\n",
    "num_list = [num_label_dict[x] for x in requested_label_lst]\n",
    "y_updated = np.array(num_list)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_updated = X_train[requested_index_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, n_estimators=100, max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "\n",
    "    def fit(self, X_updated, y_updated):\n",
    "        with mlflow.start_run():\n",
    "            self.rf.fit(X_updated, y_updated)\n",
    "            mlflow.sklearn.log_model(self.rf, \"random-forest-model\")\n",
    "            \n",
    "    def predict(self, context, model_input):\n",
    "        return self.choices(self.labels, k=len(model_input))\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.labels = ['Engine / Hybrid System', 'Audio / Visual / Telematics', 'Brake',\n",
    "                       'Vehicle Interior', 'Suspension', 'Vehicle Exterior', 'Drivetrain',\n",
    "                       'Power Source / Network', 'Steering', 'General', 'ADAS / AD']\n",
    "        from random import choices \n",
    "        self.choices = choices\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'custom-model' already exists. Creating a new version of this model...\n",
      "2023/03/12 02:25:38 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: custom-model, version 4\n",
      "Created version '4' of model 'custom-model'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Test Custom model\") as run:\n",
    "    model = RandomForest()\n",
    "\n",
    "    mlflow.log_param(\"param_1\", 3)\n",
    "    mlflow.log_metrics({\"metric_1\": 2, \"metric_2\": 2})\n",
    "\n",
    "    # Log the sklearn model and register as version 1\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"custom-model\",\n",
    "        python_model=model,\n",
    "        code_path = [\".\"],\n",
    "        registered_model_name=\"custom-model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1678542118711, current_stage='Production', description='', last_updated_timestamp=1678587973471, name='custom-model', run_id='bcfdc5da3b004163a18a44dc441ff7a3', run_link='', source='gs://novhack2023_mlflow_bucket_team-11/0/bcfdc5da3b004163a18a44dc441ff7a3/artifacts/custom-model', status='READY', status_message='', tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"custom-model\",\n",
    "    version=1,\n",
    "    stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [11321, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model to the training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclf_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_updated\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_updated\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m-> 1124\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [11321, 2]"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "clf_rf.fit(X_updated,y_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "X5niAIWZIk6_",
    "outputId": "bea56783-0aa2-482f-f9eb-5b37edbf8bdb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# random initialization\n",
    "import random\n",
    "\n",
    "# Generate 20 random integers between 0 and 99\n",
    "n = 20\n",
    "rand_ints = random.sample(range(401,800), n)\n",
    "X_init_train_temp = X_all[rand_ints]\n",
    "X_init_train = glove_embedding(X_init_train_temp.tolist())\n",
    "\n",
    "y_init_train = y_all[rand_ints]\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf_rf.fit(X_init_train,y_init_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "nRj_a2oitgxa"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request labels\n",
    "def update(X_train,indice_lst):\n",
    "    label_lst = []\n",
    "    for index in indice_lst:\n",
    "        label = get_label(index)\n",
    "        # update the dataset\n",
    "        label_lst.append(label) \n",
    "        \n",
    "    # update current label repo\n",
    "    result = previous_label()\n",
    "    # clean the labels\n",
    "    requested_index_lst = []\n",
    "    requested_label_lst = []\n",
    "    pair_lst = result.split(',')\n",
    "    for i in pair_lst:\n",
    "        index = int(i.split(':')[0].split('\"')[1])\n",
    "        label = i.split(':')[1].split('\"')[1]\n",
    "        requested_index_lst.append(index)\n",
    "        requested_label_lst.append(label)\n",
    "        \n",
    "    X_updated = X_train[requested_index_lst]\n",
    "    y_updated = requested_label_lst\n",
    "    \n",
    "    return X_updated, y_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_311/3896295590.py:10: RuntimeWarning: divide by zero encountered in log2\n",
      "  expected_information_gain = entropy_scores - np.sum(probas * np.log2(probas), axis=1)\n",
      "/tmp/ipykernel_311/3896295590.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  expected_information_gain = entropy_scores - np.sum(probas * np.log2(probas), axis=1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [11321, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m X_updated, y_updated \u001b[38;5;241m=\u001b[39m (X_train,q_indices)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# retrain the labels for the current query to the active learner.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mclf_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_updated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_updated\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m-> 1124\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [11321, 2]"
     ]
    }
   ],
   "source": [
    "# backup code for training\n",
    "num_queries = 2\n",
    "n = 2\n",
    "for i in range(num_queries):\n",
    "    # ...where each iteration consists of labelling 20 samples\n",
    "    # ensure the requested instances do not occur with the previously requested ones\n",
    "\n",
    "    # change this into majority voting\n",
    "    #sorted, q_indices = uncertainty_search(X_train,n,clf_rf)\n",
    "    q_indices = vote_index(X_train,n,clf_rf)\n",
    "    \n",
    "    X_updated, y_updated = (X_train,q_indices)\n",
    "    \n",
    "    # retrain the labels for the current query to the active learner.\n",
    "    clf_rf.fit(X_updated, y_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5406, 7143]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11321, 300)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_updated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "DZ6IH6m2yC7M",
    "outputId": "d2e89f3a-8f4a-4753-a1c2-3852af4a6173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "0.24\n",
      "---------------\n",
      "0.24\n",
      "---------------\n",
      "0.24\n",
      "---------------\n",
      "0.24\n",
      "---------------\n",
      "0.24\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# backup code for training\n",
    "num_queries = 5\n",
    "n = 2\n",
    "for i in range(num_queries):\n",
    "    # ...where each iteration consists of labelling 20 samples\n",
    "    # ensure the requested instances do not occur with the previously requested ones\n",
    "\n",
    "    # change this into majority voting\n",
    "    #sorted, q_indices = uncertainty_search(X_train,n,clf_rf)\n",
    "    q_indices = vote_index(X_train,n,clf_rf)\n",
    "    \n",
    "    # request labels\n",
    "    X_labeled_train = X_train[q_indices]\n",
    "    y_labeled_train = y_train[q_indices]\n",
    "\n",
    "    if i > 0:\n",
    "      X_labeled_train_all = np.concatenate((X_labeled_train_all,X_labeled_train))\n",
    "      y_labeled_train_all = np.concatenate((y_labeled_train_all,y_labeled_train))\n",
    "    \n",
    "    else:\n",
    "      X_labeled_train_all = X_train[q_indices]\n",
    "      y_labeled_train_all = y_train[q_indices]\n",
    "\n",
    "    # retrain the labels for the current query to the active learner.\n",
    "    clf_rf.fit(X_labeled_train_all, y_labeled_train_all)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
